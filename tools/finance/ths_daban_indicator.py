#!/usr/bin/env python3
"""
Ths Daban Indicator Service - åŒèŠ±é¡ºæ‰“æ¿æŒ‡æ ‡æœåŠ¡
åŸºäºTushare Pro APIï¼Œæ•´åˆæ‰“æ¿ç­–ç•¥æ ¸å¿ƒå› å­
"""
import logging
import asyncio
import os
import json
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional
import pandas as pd
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

try:
    import tushare as ts
    TUSHARE_AVAILABLE = True
except ImportError:
    TUSHARE_AVAILABLE = False
    logging.warning("tushare not installed. Service will not be available.")

class ThsDabanService:
    """
    åŒèŠ±é¡ºæ‰“æ¿æŒ‡æ ‡æœåŠ¡
    æ ¸å¿ƒåŠŸèƒ½ï¼šæä¾›å…¨æ–¹ä½çš„æ‰“æ¿å†³ç­–å› å­ï¼ŒåŒ…æ‹¬å°æ¿æ—¶é—´ã€å°å•åŠ›åº¦ã€æ¢æ‰‹ç‡ã€èµ„é‡‘æµå‘ã€æ¿å—åœ°ä½åŠå¸‚åœºæƒ…ç»ªç­‰ã€‚
    """
    
    # æ¦‚å¿µè¿‡æ»¤é»‘åå• (è¿‡æ»¤æ‰æ³›é‡‘èå±æ€§ã€æŒ‡æ•°æˆåˆ†ç­‰éé¢˜æç±»æ¦‚å¿µ)
    CONCEPT_IGNORE_LIST = [
        'èèµ„èåˆ¸', 'è½¬èåˆ¸æ ‡çš„', 'èèµ„æ ‡çš„', 'èåˆ¸æ ‡çš„', 'èèµ„æ ‡çš„è‚¡', 'èåˆ¸æ ‡çš„è‚¡',
        'æ·±è‚¡é€š', 'æ²ªè‚¡é€š', 'æ¸¯è‚¡é€š', 'HS300_', 'SZ50_', 'ZZ500_',
        'æ ‡æ™®é“ç¼æ–¯', 'å¯Œæ—¶ç½—ç´ ', 'MSCI', 'è¯é‡‘æŒè‚¡', 'æ±‡é‡‘æŒè‚¡', 'ç¤¾ä¿é‡ä»“',
        'åŸºé‡‘é‡ä»“', 'QFIIé‡ä»“', 'æˆä»½è‚¡', 'æŒ‡æ•°', 'æ¿å—', 'å«å¯è½¬å€º', 'æ–°è‚¡ä¸æ¬¡æ–°è‚¡'
    ]
    
    def __init__(self, token: Optional[str] = None):
        self.name = "Ths Daban Indicator Service"
        self.token = token or os.getenv('TUSHARE_TOKEN')
        self.pro = None
        self.yuzi_map = {} # name -> {desc, orgs}
        self.yuzi_list = [] # [name1, name2, ...]
        
        # Cache settings
        self.cache_dir = Path("data/cache/tushare")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_expire_hours = 24  # Cache expiration time
        
        if not self.token:
             logging.warning("TUSHARE_TOKEN not found in environment variables")

        if TUSHARE_AVAILABLE:
            try:
                if self.token:
                    ts.set_token(self.token)
                    self.pro = ts.pro_api(self.token)
                    logging.info("ThsDabanService initialized")
                    self._init_yuzi_data()
                else:
                    logging.warning("ThsDabanService initialized without token")
            except Exception as e:
                logging.error(f"Failed to initialize Tushare API: {str(e)}")
    
    def _load_cache(self, cache_key: str) -> Optional[Any]:
        """Load data from cache if valid"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    cache_time = cache_data.get('timestamp', 0)
                    if time.time() - cache_time < self.cache_expire_hours * 3600:
                        logging.info(f"âœ… Loaded {cache_key} from cache")
                        return cache_data.get('data')
            except Exception as e:
                logging.warning(f"Cache read error: {e}")
        return None
    
    def _save_cache(self, cache_key: str, data: Any):
        """Save data to cache"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            cache_data = {
                'timestamp': time.time(),
                'data': data
            }
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ’¾ Saved {cache_key} to cache")
        except Exception as e:
            logging.warning(f"Cache write error: {e}")
    
    def _init_yuzi_data(self):
        """Initialize Yuzi (hot money) data with cache support"""
        # Try load from cache first
        cached_data = self._load_cache('yuzi_map')
        if cached_data:
            self.yuzi_map = cached_data.get('yuzi_map', {})
            self.yuzi_list = cached_data.get('yuzi_list', [])
            logging.info(f"Loaded {len(self.yuzi_list)} Yuzi profiles from cache.")
            return
        
        try:
            logging.info("Fetching Yuzi list (hm_list) from API...")
            df = self.pro.hm_list()
            if df is not None and not df.empty:
                for _, row in df.iterrows():
                    name = row.get('name') or row.get('hm_name')
                    if not name:
                        continue
                    
                    self.yuzi_map[name] = {
                        "desc": row.get('desc', 'æš‚æ— æè¿°'),
                        "orgs": row.get('orgs', '')
                    }
                    self.yuzi_list.append(name)
                logging.info(f"Loaded {len(self.yuzi_list)} Yuzi profiles from API.")
                
                # Save to cache
                self._save_cache('yuzi_map', {
                    'yuzi_map': self.yuzi_map,
                    'yuzi_list': self.yuzi_list
                })
            else:
                logging.warning("hm_list returned empty.")
        except Exception as e:
            logging.warning(f"Failed to fetch hm_list (Check permissions or rate limit): {e}")
    
    async def _get_code_by_name(self, names: List[str]) -> Dict[str, str]:
        """Get stock codes by names with cache support"""
        if not self.pro:
            return {}
        
        name_map = {}
        
        # Try load from cache first
        cached_data = self._load_cache('stock_basic')
        df = None
        
        if cached_data:
            try:
                df = pd.DataFrame(cached_data)
            except Exception:
                pass
        
        # Fetch from API if cache miss
        if df is None or df.empty:
            try:
                loop = asyncio.get_running_loop()
                df = await loop.run_in_executor(None, lambda: self.pro.stock_basic(exchange='', list_status='L', fields='ts_code,symbol,name'))
                
                if df is not None and not df.empty:
                    # Save to cache
                    self._save_cache('stock_basic', df.to_dict('records'))
            except Exception as e:
                logging.error(f"Error fetching stock_basic: {e}")
                return {}
        
        # Map names to codes
        if df is not None and not df.empty:
            for name in names:
                row = df[df['name'] == name]
                if not row.empty:
                    name_map[name] = row.iloc[0]['ts_code']
        
        return name_map

    async def _fetch_yuzi_detail(self, trade_date: str, ts_code: str, hm_name: str, semaphore: asyncio.Semaphore) -> Optional[dict]:
        async with semaphore:
            try:
                loop = asyncio.get_running_loop()
                df = await loop.run_in_executor(None, lambda: self.pro.hm_detail(trade_date=trade_date, ts_code=ts_code, hm_name=hm_name))
                if df is not None and not df.empty:
                    return df.iloc[0].to_dict()
            except Exception:
                pass 
            return None

    async def _fetch_limit_minute_amount(self, ts_code: str, trade_date: str, first_time: str) -> str:
        if not first_time or len(str(first_time)) < 4:
            return "æ— å°æ¿æ—¶é—´"
        try:
            ft_str = str(first_time).zfill(6)
            hh, mm = ft_str[0:2], ft_str[2:4]
            trade_dt_str = f"{trade_date[:4]}-{trade_date[4:6]}-{trade_date[6:8]}"
            start_dt = f"{trade_dt_str} {hh}:{mm}:00"
            end_dt = f"{trade_dt_str} {hh}:{mm}:59"
            
            loop = asyncio.get_running_loop()
            df = await loop.run_in_executor(None, lambda: self.pro.stk_mins(ts_code=ts_code, freq='1min', start_date=start_dt, end_date=end_dt))
            
            if df is not None and not df.empty:
                return f"{round(float(df.iloc[0]['amount'])/10000, 2)}ä¸‡"
            else:
                return "æ— åˆ†é’Ÿæ•°æ®"
        except Exception as e:
            logging.warning(f"Minute data fetch failed: {e}")
            return "æƒé™/æ•°æ®é”™è¯¯"

    async def _fetch_stock_concepts(self, ts_code: str) -> str:
        try:
            loop = asyncio.get_running_loop()
            df = await loop.run_in_executor(None, lambda: self.pro.concept_detail(ts_code=ts_code))
            if df is not None and not df.empty:
                concepts = df['concept_name'].unique().tolist()
                filtered_concepts = []
                for c in concepts:
                    is_ignored = False
                    for ignore in self.CONCEPT_IGNORE_LIST:
                        if ignore in c:
                            is_ignored = True
                            break
                    if not is_ignored:
                        filtered_concepts.append(c)
                return ",".join(filtered_concepts[:10]) 
            return ""
        except Exception:
            return ""

    async def _fetch_ths_concepts(self, ts_code: str) -> List[Dict[str, str]]:
        """è·å–ä¸ªè‚¡æ‰€å±çš„åŒèŠ±é¡ºæ¦‚å¿µæ¿å—"""
        try:
            loop = asyncio.get_running_loop()
            df = await loop.run_in_executor(None, lambda: self.pro.ths_member(con_code=ts_code))
            if df is not None and not df.empty:
                concepts = []
                for _, row in df.iterrows():
                    if row.get('ts_code'):
                        concepts.append({'code': row['ts_code'], 'name': 'åŠ è½½ä¸­'}) 
                return concepts
            return []
        except Exception as e:
            logging.warning(f"ths_member failed: {e}")
            return []

    async def _fetch_sector_members(self, sector_code: str) -> List[str]:
        """è·å–æ¿å—æˆåˆ†è‚¡åˆ—è¡¨"""
        try:
            loop = asyncio.get_running_loop()
            df = await loop.run_in_executor(None, lambda: self.pro.ths_member(ts_code=sector_code))
            if df is not None and not df.empty:
                return df['con_code'].tolist()
            return []
        except Exception:
            return []

    async def _fetch_stock_hot_rank(self, trade_date: str, stock_names: List[str]) -> Dict[str, Any]:
        """è·å–ä¸ªè‚¡åœ¨åŒèŠ±é¡ºçƒ­åº¦æ¦œçš„æ’å (æ•´åˆè‡ª ThsHotService)"""
        if not self.pro:
            return {}
        try:
            loop = asyncio.get_running_loop()
            # è·å–å…¨å¸‚åœºçƒ­åº¦æ¦œ
            df = await loop.run_in_executor(
                None, 
                lambda: self.pro.ths_hot(trade_date=trade_date, market='çƒ­è‚¡', fields='ts_code,ts_name,hot,rank')
            )
            
            result = {}
            if df is not None and not df.empty:
                for name in stock_names:
                    # æ¨¡ç³ŠåŒ¹é…æˆ–ç²¾ç¡®åŒ¹é…
                    row = df[df['ts_name'] == name]
                    if not row.empty:
                        item = row.iloc[0]
                        result[name] = {
                            'hot': item.get('hot'),
                            'rank': item.get('rank')
                        }
                    else:
                        result[name] = {'hot': None, 'rank': 'æœªä¸Šæ¦œ'}
            return result
        except Exception as e:
            logging.warning(f"Failed to fetch hot rank: {e}")
            return {}
            
    async def _fetch_auction_data(self, ts_code: str, trade_date: str) -> Dict[str, Any]:
        """è·å–ä¸ªè‚¡å½“æ—¥é›†åˆç«ä»·æ•°æ® (stk_auction)"""
        if not self.pro:
            return {}
        try:
            loop = asyncio.get_running_loop()
            df = await loop.run_in_executor(
                None,
                lambda: self.pro.stk_auction(ts_code=ts_code, trade_date=trade_date, fields='vol,price,amount,turnover_rate,volume_ratio')
            )
            if df is not None and not df.empty:
                # ç†è®ºä¸Šåªæœ‰ä¸€è¡Œæ•°æ®ï¼Œä½†å¦‚æœåˆ†ç¬”ä¼šæœ‰å¤šè¡Œï¼Œstk_auction è¿”å›çš„æ˜¯å½“æ—¥æ±‡æ€»è¿˜æ˜¯æ˜ç»†ï¼Ÿ
                # æ ¹æ®ç¤ºä¾‹ï¼Œæ¯å¤©ä¸€æ¡è®°å½•
                row = df.iloc[0]
                return {
                    "open_price": float(row['price']) if pd.notna(row['price']) else 0,
                    "auction_amount": float(row['amount']) if pd.notna(row['amount']) else 0,
                    "auction_turnover": float(row['turnover_rate']) if pd.notna(row['turnover_rate']) else 0,
                    "auction_vol_ratio": float(row['volume_ratio']) if pd.notna(row['volume_ratio']) else 0,
                    "found": True
                }
            return {"found": False}
        except Exception as e:
            logging.warning(f"Fetch auction data failed: {e}")
            return {"found": False}

    async def _fetch_cyq_chips(self, ts_code: str, trade_date: str) -> Optional[pd.DataFrame]:
        """è·å–å®˜æ–¹ç­¹ç åˆ†å¸ƒæ•°æ® (cyq_chips)"""
        if not self.pro:
            return None
        try:
            loop = asyncio.get_running_loop()
            df = await loop.run_in_executor(
                None,
                lambda: self.pro.cyq_chips(ts_code=ts_code, trade_date=trade_date)
            )
            if df is not None and not df.empty:
                return df
            return None
        except Exception as e:
            logging.warning(f"Fetch cyq_chips failed: {e}")
            return None

    def _process_cyq_data(self, df_cyq: pd.DataFrame, current_price: float) -> Dict[str, Any]:
        """å¤„ç†å®˜æ–¹ç­¹ç æ•°æ®ï¼Œè®¡ç®—æ ¸å¿ƒæŒ‡æ ‡"""
        if df_cyq is None or df_cyq.empty:
            return {}
            
        # df columns: price, percent
        # Normalize percent just in case (sum should be 100 or 1)
        total_pct = df_cyq['percent'].sum()
        if total_pct == 0:
            return {}
        
        sorted_chips = df_cyq.sort_values('price')
        
        # 1. è·åˆ©ç›˜: price < current_price
        profit_df = sorted_chips[sorted_chips['price'] < current_price]
        profit_rate = (profit_df['percent'].sum() / total_pct) * 100
        
        # 2. å¹³å‡æˆæœ¬
        avg_cost = (sorted_chips['price'] * sorted_chips['percent']).sum() / total_pct
        
        # 3. æˆæœ¬é›†ä¸­åº¦
        # Calculate CDF to find cost_15 and cost_85
        sorted_chips['cumsum_pct'] = sorted_chips['percent'].cumsum() / total_pct
        
        cost_15_series = sorted_chips[sorted_chips['cumsum_pct'] >= 0.15]
        cost_85_series = sorted_chips[sorted_chips['cumsum_pct'] >= 0.85]
        
        cost_15 = cost_15_series['price'].iloc[0] if not cost_15_series.empty else 0
        cost_85 = cost_85_series['price'].iloc[0] if not cost_85_series.empty else 0
        
        concentration = 100
        if (cost_85 + cost_15) > 0:
            concentration = (cost_85 - cost_15) / (cost_85 + cost_15) * 100
            
        return {
            "è·åˆ©ç›˜": round(profit_rate, 2),
            "å¹³å‡æˆæœ¬": round(avg_cost, 2),
            "ç­¹ç é›†ä¸­åº¦": round(concentration, 2),
            "source": "çœŸå®ç­¹ç (CYQ)"
        }

    def _estimate_chip_distribution_algo(self, df_history: pd.DataFrame, current_price: float) -> Dict[str, Any]:
        """
        [Fallback] ç®—æ³•ä¼°ç®—ç­¹ç åˆ†å¸ƒ
        åŸºäºå†å²æ¢æ‰‹ç‡å’Œå‡ä»·è¿›è¡Œç­¹ç è¡°å‡è®¡ç®—
        """
        if df_history is None or df_history.empty:
            return {"è·åˆ©ç›˜": 0, "å¹³å‡æˆæœ¬": 0, "90%æˆæœ¬åŒºé—´": "æ— æ•°æ®", "ç­¹ç é›†ä¸­åº¦": 0, "source": "æ— æ•°æ®"}

        # ç¡®ä¿æŒ‰æ—¥æœŸå‡åº
        df = df_history.sort_values('trade_date', ascending=True).reset_index(drop=True)
        
        # ç®€åŒ–ç‰ˆç­¹ç ç®—æ³•
        chip_buckets = {}
        
        # æ ¸å¿ƒè¿­ä»£ (å‡è®¾æœ€è¿‘ 120 å¤©)
        if len(df) > 120:
            df = df.iloc[-120:]
            
        for _, row in df.iterrows():
            try:
                turnover = float(row['turnover_rate']) / 100.0
                if pd.isna(turnover):
                    continue
                
                vol = float(row.get('vol', 0))
                amount = float(row.get('amount', 0))
                avg_price = (amount * 10 / vol) if vol > 0 else row['close']
                
                decay = 1.0 - turnover
                keys = list(chip_buckets.keys())
                for p in keys:
                    chip_buckets[p] *= decay
                
                price_key = round(avg_price, 2)
                chip_buckets[price_key] = chip_buckets.get(price_key, 0.0) + turnover
            except Exception:
                continue
        
        total_mass = sum(chip_buckets.values())
        if total_mass == 0:
            return {"è·åˆ©ç›˜": 0, "å¹³å‡æˆæœ¬": 0, "90%æˆæœ¬åŒºé—´": "æ— æ•°æ®", "ç­¹ç é›†ä¸­åº¦": 0, "source": "ä¼°ç®—å¤±è´¥"}
        
        sorted_chips = sorted(chip_buckets.items(), key=lambda x: x[0])
        
        profit_mass = sum(mass for p, mass in sorted_chips if p < current_price)
        profit_rate = (profit_mass / total_mass) * 100
        
        weighted_sum = sum(p * mass for p, mass in sorted_chips)
        avg_cost = weighted_sum / total_mass
        
        cum_mass = 0
        cost_5 = 0
        cost_95 = 0
        cost_15 = 0
        cost_85 = 0
        
        for p, mass in sorted_chips:
            cum_mass += mass
            ratio = cum_mass / total_mass
            if cost_5 == 0 and ratio >= 0.05:
                cost_5 = p
            if cost_15 == 0 and ratio >= 0.15:
                cost_15 = p
            if cost_85 == 0 and ratio >= 0.85:
                cost_85 = p
            if cost_95 == 0 and ratio >= 0.95:
                cost_95 = p
            
        concentration = 100
        if (cost_85 + cost_15) > 0:
            concentration = (cost_85 - cost_15) / (cost_85 + cost_15) * 100
            
        return {
            "è·åˆ©ç›˜": round(profit_rate, 2),
            "å¹³å‡æˆæœ¬": round(avg_cost, 2),
            "90%æˆæœ¬åŒºé—´": f"{round(cost_5, 2)} - {round(cost_95, 2)}",
            "ç­¹ç é›†ä¸­åº¦": round(concentration, 2),
            "source": "ä¼°ç®—ç­¹ç (Algo)"
        }

    def _calculate_market_sentiment(self, df_limit: pd.DataFrame) -> Dict[str, Any]:
        """
        è®¡ç®—å¸‚åœºè¿æ¿æƒ…ç»ªæŒ‡æ ‡
        åŸºäºå½“æ—¥æ¶¨åœåˆ—è¡¨è®¡ç®—ï¼š
        1. æœ€é«˜è¿æ¿é«˜åº¦
        2. æ¶¨åœæ€»å®¶æ•°
        3. è¿æ¿å®¶æ•° (è¿æ¿æ•°>=2)
        """
        sentiment = {
            "æœ€é«˜è¿æ¿é«˜åº¦": 0,
            "æ¶¨åœæ€»å®¶æ•°": 0,
            "è¿æ¿å®¶æ•°": 0,
            "æƒ…ç»ªæè¿°": "æ— æ³•è®¡ç®—"
        }
        
        if df_limit is None or df_limit.empty:
            return sentiment
            
        try:
            # è¿‡æ»¤æ‰éæ¶¨åœçŠ¶æ€ (å‡è®¾ limit_type='U' ä¸” open_times=0 æˆ–è€…æ˜¯æœ€åçŠ¶æ€æ˜¯å°æ¿)
            # Tushare limit_list_d é€šå¸¸è¿”å›å½“æ—¥æœ‰è¿‡æ¶¨åœçš„ï¼Œæˆ‘ä»¬éœ€è¦è¿‡æ»¤æ”¶ç›˜å°ä½çš„
            # è¿™é‡Œç®€å•å¤„ç†ï¼Œç»Ÿè®¡æ‰€æœ‰è®°å½•
            
            sentiment["æ¶¨åœæ€»å®¶æ•°"] = len(df_limit)
            
            if 'limit_times' in df_limit.columns:
                df_limit['limit_times'] = df_limit['limit_times'].fillna(1).astype(int)
                sentiment["æœ€é«˜è¿æ¿é«˜åº¦"] = df_limit['limit_times'].max()
                sentiment["è¿æ¿å®¶æ•°"] = len(df_limit[df_limit['limit_times'] >= 2])
            
            # ç®€å•æƒ…ç»ªè¯„çº§
            limit_count = sentiment["æ¶¨åœæ€»å®¶æ•°"]
            height = sentiment["æœ€é«˜è¿æ¿é«˜åº¦"]
            
            if limit_count > 100 and height >= 5:
                desc = "æƒ…ç»ªç«çˆ†"
            elif limit_count > 50 and height >= 3:
                desc = "æƒ…ç»ªå°šå¯"
            else:
                desc = "æƒ…ç»ªä½è¿·"
                
            sentiment["æƒ…ç»ªæè¿°"] = f"{desc} (æ¶¨åœ{limit_count}å®¶, æœ€é«˜{height}æ¿, è¿æ¿{sentiment['è¿æ¿å®¶æ•°']}å®¶)"
            
        except Exception as e:
            logging.warning(f"Sentiment calculation error: {e}")
            
        return sentiment

    async def _analyze_yesterday_premium(self, target_date: str) -> Dict[str, Any]:
        """
        åˆ†ææ˜¨æ—¥æ¶¨åœè‚¡çš„ä»Šæ—¥è¡¨ç° (èµšé’±æ•ˆåº”/æ¥åŠ›æƒ…ç»ª)
        åˆ©ç”¨ limit_list_ths è·å–æ˜¨æ—¥æ¶¨åœï¼Œdaily è·å–ä»Šæ—¥æ¶¨å¹…
        """
        if not self.pro:
            return {}
        
        try:
            loop = asyncio.get_running_loop()
            
            # 1. è®¡ç®—ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥
            df_cal = await loop.run_in_executor(None, lambda: self.pro.trade_cal(exchange='', end_date=target_date, is_open='1', limit=5))
            if df_cal is None or df_cal.empty or len(df_cal) < 2:
                return {"æè¿°": "æ— äº¤æ˜“æ—¥å†æ•°æ®"}
            
            df_cal = df_cal.sort_values('cal_date', ascending=False)
            prev_date = df_cal.iloc[1]['cal_date'] # Index 0 æ˜¯ target_date
            
            # 2. è·å–æ˜¨æ—¥æ¶¨åœæ± 
            df_prev_limit = await loop.run_in_executor(
                None, 
                lambda: self.pro.limit_list_ths(trade_date=prev_date, limit_type='æ¶¨åœæ± ', fields='ts_code,name')
            )
            
            if df_prev_limit is None or df_prev_limit.empty:
                return {"æè¿°": f"æ˜¨æ—¥({prev_date})æ— æ¶¨åœæ•°æ®"}
            
            prev_codes = df_prev_limit['ts_code'].tolist()
            prev_limit_count = len(prev_codes)
            
            # 3. è·å–è¿™äº›è‚¡ç¥¨ä»Šæ—¥è¡Œæƒ…
            today_performance = []
            chunk_size = 500
            for i in range(0, len(prev_codes), chunk_size):
                chunk = prev_codes[i:i+chunk_size]
                chunk_str = ",".join(chunk)
                df_today = await loop.run_in_executor(
                    None,
                    lambda: self.pro.daily(trade_date=target_date, ts_code=chunk_str, fields='ts_code,pct_chg,close')
                )
                if df_today is not None and not df_today.empty:
                    today_performance.append(df_today)
            
            if not today_performance:
                return {"æè¿°": "æ˜¨æ—¥æ¶¨åœè‚¡ä»Šæ—¥æ— è¡Œæƒ…æ•°æ®"}
                
            df_today_all = pd.concat(today_performance)
            
            # 4. è®¡ç®—æŒ‡æ ‡
            avg_pct = df_today_all['pct_chg'].mean()
            up_count = len(df_today_all[df_today_all['pct_chg'] > 0])
            # æ™‹çº§ç‡ (ç®€å•ç”¨ > 9.5% ä¼°ç®—)
            limit_count = len(df_today_all[df_today_all['pct_chg'] > 9.5]) 
            
            promotion_rate = (limit_count / prev_limit_count) * 100
            win_rate = (up_count / len(df_today_all)) * 100
            
            desc = f"æ˜¨æ—¥æ¶¨åœ{prev_limit_count}å®¶, ä»Šæ—¥å¹³å‡æ¶¨å¹…{round(avg_pct, 2)}%, æ™‹çº§ç‡{round(promotion_rate, 2)}%({limit_count}å®¶), èµšé’±æ•ˆåº”{round(win_rate, 2)}%"
            
            return {
                "å¹³å‡æ¶¨å¹…": round(avg_pct, 2),
                "æ™‹çº§ç‡": round(promotion_rate, 2),
                "èµšé’±æ•ˆåº”": round(win_rate, 2),
                "æè¿°": desc
            }
            
        except Exception as e:
            logging.warning(f"Yesterday premium analysis failed: {e}")
            return {"æè¿°": "è®¡ç®—å¤±è´¥"}

    async def get_daban_indicators(self, stock_names: str, date: str) -> Dict[str, Any]:
        """
        è·å–æ‰“æ¿æ ¸å¿ƒå› å­ (Core Strategy Function)
        
        Args:
            stock_names: è‚¡ç¥¨åç§°ï¼Œæ”¯æŒå¤šä¸ªï¼Œé€—å·åˆ†éš” (e.g., 'åˆ©æ¬§è‚¡ä»½,ä¸­ä¿¡è¯åˆ¸').
            date: æŸ¥è¯¢æ—¥æœŸï¼Œæ ¼å¼ 'YYYY-MM-DD' æˆ– 'YYYYMMDD'.
            
        Returns:
            Dict: åŒ…å«ä¸ªè‚¡æ‰“æ¿å› å­çš„è¯¦ç»†æ•°æ®å­—å…¸.
                  - success (bool): æ˜¯å¦æˆåŠŸ
                  - data (list): ä¸ªè‚¡æŒ‡æ ‡åˆ—è¡¨
                  - metadata (dict): å…ƒæ•°æ®

        | å› å­åç§° | å«ä¹‰ä¸ä½œç”¨ |
        |----------|------------|
        | æ˜¨æ—¥æ¶¨åœè¡¨ç° | å¸‚åœºæ¥åŠ›æƒ…ç»ªæŒ‡æ ‡ï¼šåŒ…æ‹¬æ˜¨æ—¥æ¶¨åœè‚¡ä»Šæ—¥å¹³å‡æ¶¨å¹…ã€æ™‹çº§ç‡ã€èµšé’±æ•ˆåº” |
        | ç«ä»·é‡‘é¢/æ¢æ‰‹ | é›†åˆç«ä»·é˜¶æ®µçš„æˆäº¤é¢åŠæ¢æ‰‹ç‡ï¼ˆåˆ¤æ–­å¼€ç›˜æŠ¢ç­¹çƒ­åº¦ï¼‰ |
        | ç«ä»·é‡æ¯” | é›†åˆç«ä»·æˆäº¤é‡ä¸è¿‡å»5æ—¥å‡é‡çš„æ¯”å€¼ï¼ˆ>5ä¸ºçˆ†é‡ï¼Œä¸»åŠ›å¼‚åŠ¨æ˜æ˜¾ï¼‰ |
        | è·åˆ©ç›˜æ¯”ä¾‹ | å½“å‰ä»·æ ¼ä¸‹å¤„äºç›ˆåˆ©çŠ¶æ€çš„ç­¹ç æ¯”ä¾‹ï¼ˆ>90%ä¸ºæ–°é«˜æ¿ï¼Œä¸Šæ–¹æ— å‹åŠ›ï¼‰ |
        | ç­¹ç é›†ä¸­åº¦ | 90%ç­¹ç åˆ†å¸ƒçš„å¯†é›†ç¨‹åº¦ï¼ˆæ•°å€¼è¶Šå°è¶Šé›†ä¸­ï¼Œ<10%ä¸ºé«˜åº¦æ§ç›˜ï¼‰ |
        | å¹³å‡æˆæœ¬ | å¸‚åœºå¹³å‡æŒä»“æˆæœ¬ |
        | é¦–æ¿å°æ¿æ—¶é—´ | ç¬¬ä¸€æ¿æ¶¨åœæ—¶é—´ç‚¹ï¼ˆè¶Šæ—©è¶Šå¥½ï¼Œåæ˜ é¢˜æå¼ºåº¦å’Œèµ„é‡‘åŠ›åº¦ï¼‰ |
        | äºŒæ¿å°æ¿æ—¶é—´ | ç¬¬äºŒæ¿æ¶¨åœæ—¶é—´ç‚¹ï¼ˆç»¼åˆä¸¤å¤©ï¼Œè§‚å¯Ÿæœ‰æ— åŠ é€Ÿå°æ¿è¿¹è±¡ï¼‰ |
        | å°æ¿æ¬¡æ•°/ç‚¸æ¿æ¬¡æ•° | äºŒæ¿å½“æ—¥å°æ¿è¢«æ‰“å¼€çš„æ¬¡æ•°ï¼ˆ0æ¬¡æœ€å¥½ï¼Œæ¬¡æ•°å¤šåˆ™å°æ¿è´¨é‡å·®ï¼‰ |
        | äºŒæ¿å°å•æ¯”ä¾‹ | æ”¶ç›˜æ—¶æ¶¨åœå°å•æ‰‹æ•°æˆ–é‡‘é¢ä¸å½“æ—¥æˆäº¤çš„æ¯”å€¼ï¼ˆè¶Šé«˜è¡¨ç¤ºå°æ¿å¼ºåº¦è¶Šå¤§ï¼‰|
        | äºŒæ¿æœ€é«˜å°å•é‡‘é¢/æµé€šå¸‚å€¼ | è¶Šå¤§è¯´æ˜å°å•é‡‘é¢ç›¸å¯¹å¸‚å€¼è¶Šå¤§ï¼Œæœ‰åˆ©äºè¿æ¿|
        | é¦–æ¿æ¢æ‰‹ç‡ | ç¬¬ä¸€æ¿å½“æ—¥æ¢æ‰‹ç‡ï¼ˆ%ï¼‰ï¼Œç”¨äºåˆ¤æ–­é¦–æ¿æ˜¯ç¼©é‡æ¿è¿˜æ˜¯æ”¾é‡æ¿ |
        | äºŒæ¿æ¢æ‰‹ç‡ | ç¬¬äºŒæ¿å½“æ—¥æ¢æ‰‹ç‡ï¼ˆ%ï¼‰ï¼ŒäºŒæ¿ç¼©é‡/æ”¾é‡æƒ…å†µå…³ç³»é‡å¤§ |
        | æˆäº¤é‡å˜åŒ– | äºŒæ¿æˆäº¤é¢ç›¸å¯¹äºé¦–æ¿çš„å€æ•°ï¼ˆ>1è¡¨ç¤ºæ”¾é‡ï¼Œ<1ç¼©é‡ï¼‰ |
        | é¦–æ¿æé™æˆäº¤é¢ | é¦–æ¿æ¶¨åœç¬é—´å•åˆ†é’Ÿæœ€å¤§æˆäº¤é¢ï¼ˆè¡¡é‡æ‰“æ¿æ—¶æ‰«å•åŠ›åº¦ï¼‰ |
        | é¾™è™æ¦œæœºæ„å‡€ä¹°é¢ | äºŒæ¿å½“æ—¥é¾™è™æ¦œæœºæ„å¸­ä½å‡€ä¹°å…¥é¢ï¼ˆæ­£å€¼å¤§é¢å‡€ä¹°æœ‰åˆ©äºæ™‹çº§ï¼‰ |
        | é¾™è™æ¦œçŸ¥åæ¸¸èµ„ä¹°å…¥ | æ˜¯å¦æœ‰çŸ¥åæ¸¸èµ„å¤§æ‰‹ç¬”ä¸Šæ¦œåŠå‡€ä¹°å…¥ï¼ˆæœ‰åˆ™åŠ åˆ†ï¼Œæ˜¾ç¤ºæ¸¸èµ„å…³æ³¨ï¼‰ |
        | ä¸»åŠ›èµ„é‡‘å‡€æµå…¥ | äºŒæ¿å½“æ—¥å¤§å•èµ„é‡‘å‡€æµå…¥é‡‘é¢æˆ–å æ¯”ï¼ˆåˆ¤æ–­æ˜¯å¦æœ‰ä¸»åŠ›æŒç»­åŠ ä»“ï¼‰ |
        | æ¿å—å†…è¿æ¿æ•°æ’å | ä¸ªè‚¡åœ¨æ‰€å±é¢˜ææ¿å—ä¸­çš„è¿æ¿é«˜åº¦å’Œå¼ºåº¦æ’åï¼ˆé¾™å¤´å› å­ï¼‰ |
        | æ¿å—çƒ­åº¦æŒ‡æ ‡ | æ‰€å±æ¦‚å¿µé¢˜æçš„å¸‚åœºçƒ­åº¦ï¼ˆå¦‚æ¿å—æ¶¨åœè‚¡æ•°é‡ï¼‰ |
        | å¸‚åœºè¿æ¿æƒ…ç»ª | å¤§ç›˜æƒ…ç»ªæŒ‡æ ‡ï¼ˆæ˜¨æ—¥æœ€é«˜æ¿é«˜åº¦ã€æ•´ä½“æ°›å›´ç­‰ï¼‰ |
        | æµé€šå¸‚å€¼ | æµé€šå¸‚å€¼å¤§å°ï¼ˆäº¿å…ƒï¼Œè¿‡å¤§åˆ™å‡åˆ†ï¼Œå°ç›˜æœ‰åˆ©äºè¿æ¿ï¼‰ |
        | æ”¶ç›˜å°å•é‡‘é¢/æµé€šå¸‚å€¼ | è¶Šå¤§è¯´æ˜å°å•é‡‘é¢ç›¸å¯¹å¸‚å€¼è¶Šå¤§ï¼Œæœ‰åˆ©äºè¿æ¿ |
        | ä¸ªè‚¡æ‰€å±æ¿å—è¿æ¿æ¢¯é˜Ÿåœ°ä½ | è¡¡é‡ä¸ªè‚¡åœ¨æ¿å—å†…åœ°ä½ |
        | ä¸ªè‚¡çƒ­åº¦æ’å | ä¸ªè‚¡åœ¨åŒèŠ±é¡ºçƒ­åº¦æ¦œçš„å®æ—¶æ’å |
        | ä¸ªè‚¡æ‰€å±æ¿å— | åˆ†ææ¿å—æœ¬èº«çš„çƒ­åº¦ï¼Œèµ„é‡‘ç­‰æƒ…å†µï¼Œå¥½çš„æ¿å—ä¸ªè‚¡ä¹Ÿå¥½ |
        """
        if not self.pro:
            return {"success": False, "error": "Tushare not initialized"}

        names_list = [n.strip() for n in stock_names.split(',') if n.strip()]
        if not names_list:
            return {"success": False, "error": "No stock names provided"}

        target_date = date.replace('-', '')
        name_code_map = await self._get_code_by_name(names_list)
        ts_codes = list(name_code_map.values())
        if not ts_codes:
            return {"success": False, "error": f"Could not find codes for {stock_names}"}
        
        ts_codes_str = ",".join(ts_codes)
        logging.info(f"Analyzing {names_list} ({ts_codes_str}) for {target_date}")

        loop = asyncio.get_running_loop()
        try:
             target_dt = datetime.strptime(target_date, "%Y%m%d")
             start_date = (target_dt - timedelta(days=20)).strftime("%Y%m%d")
             # ä¸ºç­¹ç è®¡ç®—å‡†å¤‡æ›´é•¿çš„å†å²æ•°æ® (120å¤©)
             history_start_date = (target_dt - timedelta(days=180)).strftime("%Y%m%d")
        except ValueError:
             return {"success": False, "error": "Invalid date format"}

        # 2. Batch Data Fetch
        try:
            # ä»·æ ¼æ•°æ® (ç”¨äºç­¹ç è®¡ç®—ï¼Œéœ€è¦è¾ƒé•¿å†å²)
            df_history_long = await loop.run_in_executor(None, lambda: self.pro.daily(ts_code=ts_codes_str, start_date=history_start_date, end_date=target_date))
            
            # ä»·æ ¼æ•°æ® (è¿‘æœŸåˆ†æ)
            df_daily_range = df_history_long[df_history_long['trade_date'] >= start_date].copy() if df_history_long is not None else None
            if df_daily_range is not None:
                df_daily_range = df_daily_range.sort_values('trade_date', ascending=True)

            # åŸºç¡€æŒ‡æ ‡
            df_basic_range = await loop.run_in_executor(None, lambda: self.pro.daily_basic(ts_code=ts_codes_str, start_date=start_date, end_date=target_date))
            if df_basic_range is not None:
                df_basic_range = df_basic_range.sort_values('trade_date', ascending=True)

            # ä¸ªè‚¡æ¶¨åœæ•°æ®
            df_limit_range = await loop.run_in_executor(None, lambda: self.pro.limit_list_d(ts_code=ts_codes_str, start_date=start_date, end_date=target_date))
            if df_limit_range is not None:
                df_limit_range = df_limit_range.sort_values('trade_date', ascending=True)
            
            # èµ„é‡‘æµå‘
            df_money_t = await loop.run_in_executor(None, lambda: self.pro.moneyflow(ts_code=ts_codes_str, trade_date=target_date))
            
            # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            df_stock_info = await loop.run_in_executor(None, lambda: self.pro.stock_basic(ts_code=ts_codes_str, fields='ts_code,industry,area,market,name'))

            # å½“æ—¥å…¨å¸‚åœºæ¶¨åœåˆ—è¡¨ (ç”¨äºæ¿å—æ’åå’Œå¸‚åœºæƒ…ç»ª)
            df_limit_all_today = await loop.run_in_executor(None, lambda: self.pro.limit_list_d(trade_date=target_date))

            # ä¸ªè‚¡çƒ­åº¦æ’å
            hot_rank_map = await self._fetch_stock_hot_rank(target_date, names_list)

            # å¸‚åœºè¿æ¿æƒ…ç»ª
            market_sentiment = self._calculate_market_sentiment(df_limit_all_today)

            # æ˜¨æ—¥æ¶¨åœè¡¨ç° (èµšé’±æ•ˆåº”)
            yesterday_premium = await self._analyze_yesterday_premium(target_date)

            # çƒ­é—¨æ¿å— (Top 5 Concept Limit Up)
            hot_sectors_map = {} 
            top5_sectors_display = [] 
            try:
                df_cpt = await loop.run_in_executor(None, lambda: self.pro.limit_cpt_list(trade_date=target_date))
                if df_cpt is not None and not df_cpt.empty:
                    if 'rank' in df_cpt.columns:
                        df_cpt = df_cpt.sort_values('rank', ascending=True)
                    else:
                        df_cpt = df_cpt.sort_values('up_nums', ascending=False)
                        
                    top5 = df_cpt.head(5)
                    for idx, row in top5.iterrows():
                        s_code = row['ts_code']
                        s_name = row['name']
                        s_rank = row.get('rank') if pd.notna(row.get('rank')) else (idx + 1)
                        
                        hot_sectors_map[s_code] = {
                            'name': s_name,
                            'rank': s_rank,
                            'desc': f"{s_name}({row['up_nums']}å®¶, {row['up_stat']})"
                        }
                        top5_sectors_display.append(f"No.{s_rank} {hot_sectors_map[s_code]['desc']}")
            except Exception as e:
                logging.warning(f"Fetch limit_cpt_list failed: {e}")

        except Exception as e:
            logging.error(f"Error fetching data batch: {e}")
            return {"success": False, "error": str(e)}

        # 3. Analysis Loop
        results = []
        for name in names_list:
            ts_code = name_code_map.get(name)
            if not ts_code:
                continue
            
            def get_t_and_prev(df, code, t_date):
                row_t, row_prev = None, None
                if df is not None and not df.empty:
                    df_code = df[df['ts_code'] == code]
                    if not df_code.empty:
                        df_code_sorted = df_code.sort_values('trade_date', ascending=True).reset_index(drop=True)
                        t_rows = df_code_sorted[df_code_sorted['trade_date'] == t_date]
                        if not t_rows.empty:
                            row_t = t_rows.iloc[0]
                            if t_rows.index[0] > 0:
                                row_prev = df_code_sorted.iloc[t_rows.index[0] - 1]
                return row_t, row_prev

            def get_single_row(df, code):
                if df is not None and not df.empty:
                    rows = df[df['ts_code'] == code]
                    if not rows.empty:
                        return rows.iloc[0]
                return None

            row_daily_t, row_daily_prev = get_t_and_prev(df_daily_range, ts_code, target_date)
            row_basic_t, row_basic_prev = get_t_and_prev(df_basic_range, ts_code, target_date)
            row_limit_t, row_limit_prev = get_t_and_prev(df_limit_range, ts_code, target_date)
            row_money_t = get_single_row(df_money_t, ts_code)
            row_info = get_single_row(df_stock_info, ts_code)
            
            # é›†åˆç«ä»·æ•°æ® fetch
            auction_data = await self._fetch_auction_data(ts_code, target_date)
            
            # ç­¹ç åˆ†å¸ƒè®¡ç®— (ä¼˜å…ˆå°è¯•å®˜æ–¹æ¥å£ cyq_chips)
            current_close = float(row_daily_t['close']) if row_daily_t is not None else 0
            chip_info = {}
            
            # Try official API first
            df_cyq_official = await self._fetch_cyq_chips(ts_code, target_date)
            if df_cyq_official is not None:
                chip_info = self._process_cyq_data(df_cyq_official, current_close)
            
            # Fallback to Algo if failed
            if not chip_info:
                df_chip_history = df_history_long[df_history_long['ts_code'] == ts_code].copy() if df_history_long is not None else None
                chip_info = self._estimate_chip_distribution_algo(df_chip_history, current_close)

            indicators = {}
            
            # --- ç«ä»·ä¸ç­¹ç  (æ–°å¢) ---
            if auction_data.get('found'):
                auc_amt = round(auction_data['auction_amount'] / 10000, 2) # ä¸‡å…ƒ
                auc_to = round(auction_data['auction_turnover'], 2)
                auc_vr = round(auction_data['auction_vol_ratio'], 2)
                indicators['ç«ä»·é‡‘é¢/æ¢æ‰‹'] = f"{auc_amt}ä¸‡ / {auc_to}%"
                indicators['ç«ä»·é‡æ¯”'] = f"{auc_vr}"
            else:
                indicators['ç«ä»·é‡‘é¢/æ¢æ‰‹'] = "æ— æ•°æ®/æœªå¼€é€šæƒé™"
                indicators['ç«ä»·é‡æ¯”'] = "-"

            indicators['è·åˆ©ç›˜æ¯”ä¾‹'] = f"{chip_info.get('è·åˆ©ç›˜', 0)}% ({chip_info.get('source', 'N/A')})"
            indicators['ç­¹ç é›†ä¸­åº¦'] = f"{chip_info.get('ç­¹ç é›†ä¸­åº¦', 0)}%"
            indicators['å¹³å‡æˆæœ¬'] = f"{chip_info.get('å¹³å‡æˆæœ¬', 0)}"
            
            # --- åŸºç¡€æ‰“æ¿å› å­ ---
            indicators['é¦–æ¿å°æ¿æ—¶é—´'] = row_limit_prev['first_time'] if row_limit_prev is not None and 'first_time' in row_limit_prev else "éæ¶¨åœ/æ— æ•°æ®"
            indicators['äºŒæ¿å°æ¿æ—¶é—´'] = row_limit_t['first_time'] if row_limit_t is not None and 'first_time' in row_limit_t else "éæ¶¨åœ/æ— æ•°æ®"
            indicators['å°æ¿æ¬¡æ•°/ç‚¸æ¿æ¬¡æ•°'] = int(row_limit_t['open_times']) if row_limit_t is not None and pd.notna(row_limit_t['open_times']) else 0
            
            fd_amount = float(row_limit_t['fd_amount']) if row_limit_t is not None and pd.notna(row_limit_t.get('fd_amount')) else 0
            total_amount = float(row_daily_t['amount']) * 1000 if row_daily_t is not None else 0 
            indicators['äºŒæ¿å°å•æ¯”ä¾‹'] = round((fd_amount / total_amount) * 100, 2) if total_amount > 0 else 0.0

            circ_mv = float(row_basic_t['circ_mv']) * 10000 if row_basic_t is not None else 0
            indicators['æµé€šå¸‚å€¼'] = round(circ_mv / 10000 / 10000, 2) if circ_mv > 0 else None
            indicators['äºŒæ¿æœ€é«˜å°å•é‡‘é¢/æµé€šå¸‚å€¼'] = round((fd_amount / circ_mv) * 100, 4) if circ_mv > 0 else 0.0
            indicators['æ”¶ç›˜å°å•é‡‘é¢/æµé€šå¸‚å€¼'] = indicators['äºŒæ¿æœ€é«˜å°å•é‡‘é¢/æµé€šå¸‚å€¼']
            
            indicators['é¦–æ¿æ¢æ‰‹ç‡'] = float(row_basic_prev['turnover_rate']) if row_basic_prev is not None else None
            indicators['äºŒæ¿æ¢æ‰‹ç‡'] = float(row_basic_t['turnover_rate']) if row_basic_t is not None else None
            
            vol_t = float(row_daily_t['vol']) if row_daily_t is not None else 0
            vol_prev = float(row_daily_prev['vol']) if row_daily_prev is not None else 0
            indicators['æˆäº¤é‡å˜åŒ–'] = round(vol_t / vol_prev, 2) if vol_prev > 0 else None
            
            limit_minute_amt = "æ— é¦–æ¿æ•°æ®"
            if row_limit_prev is not None and 'first_time' in row_limit_prev:
                limit_minute_amt = await self._fetch_limit_minute_amount(ts_code, row_limit_prev['trade_date'], row_limit_prev['first_time'])
            indicators['é¦–æ¿æé™æˆäº¤é¢'] = limit_minute_amt
            
            # --- èµ„é‡‘ä¸æ¸¸èµ„ ---
            inst_net_buy = 0.0
            yuzi_names_found = []
            semaphore = asyncio.Semaphore(5) 
            tasks = []
            
            if self.yuzi_list:
                for yuzi_name in self.yuzi_list:
                    tasks.append(self._fetch_yuzi_detail(target_date, ts_code, yuzi_name, semaphore))
                yuzi_results = await asyncio.gather(*tasks)
                
                for r in [x for x in yuzi_results if x]:
                    hm_name = r.get('hm_name', '')
                    buy_amt = float(r.get('buy_amount', 0) or 0)
                    sell_amt = float(r.get('sell_amount', 0) or 0)
                    org_name = r.get('hm_orgs') or r.get('org_name') or "æœªçŸ¥å¸­ä½"
                    
                    if 'æœºæ„' in hm_name or 'æœºæ„' in org_name:
                        inst_net_buy += (buy_amt - sell_amt)
                    
                    if buy_amt > 0:
                        style_desc = self.yuzi_map.get(hm_name, {}).get('desc', 'æš‚æ— æè¿°')
                        yuzi_names_found.append(f"ã€{hm_name}ã€‘{org_name}(ä¹°{round(buy_amt/10000, 2)}ä¸‡) [é£æ ¼:{style_desc}]")
            else:
                yuzi_names_found.append("æœªåŠ è½½æ¸¸èµ„åå½•")

            indicators['é¾™è™æ¦œæœºæ„å‡€ä¹°é¢'] = f"{round(inst_net_buy / 10000, 2)}ä¸‡"
            indicators['é¾™è™æ¦œçŸ¥åæ¸¸èµ„ä¹°å…¥'] = "\n".join(yuzi_names_found) if yuzi_names_found else "æ— /æœªè¯†åˆ«åˆ°çŸ¥åæ¸¸èµ„"

            indicators['ä¸»åŠ›èµ„é‡‘å‡€æµå…¥'] = round(float(row_money_t['buy_lg_amount'] + row_money_t['buy_elg_amount'] - row_money_t['sell_lg_amount'] - row_money_t['sell_elg_amount']), 2) if row_money_t is not None else None
            
            # --- çƒ­åº¦ä¸æƒ…ç»ª ---
            indicators['å¸‚åœºè¿æ¿æƒ…ç»ª'] = market_sentiment["æƒ…ç»ªæè¿°"]
            indicators['æ˜¨æ—¥æ¶¨åœè¡¨ç°'] = yesterday_premium.get("æè¿°", "æ— æ•°æ®")
            
            # ä¸ªè‚¡çƒ­åº¦
            hot_info = hot_rank_map.get(name, {})
            indicators['ä¸ªè‚¡çƒ­åº¦æ’å'] = f"Top {hot_info.get('rank', 'N/A')} (çƒ­åº¦å€¼: {hot_info.get('hot', 'N/A')})"
            
            # --- æ¿å—ä¸æ’å ---
            industry_name = row_info['industry'] if row_info is not None else "æœªçŸ¥"
            concept_str = await self._fetch_stock_concepts(ts_code) 
            ths_concepts = await self._fetch_ths_concepts(ts_code)
            
            matched_hot_list = []
            highest_rank_sector = None
            
            for c in ths_concepts:
                c_code = c['code']
                if c_code in hot_sectors_map:
                    hs_info = hot_sectors_map[c_code]
                    c_name = hs_info['name']
                    rank = hs_info['rank']
                    matched_hot_list.append(f"{c_name}(Top {rank})")
                    
                    if highest_rank_sector is None or rank < highest_rank_sector['rank']:
                        highest_rank_sector = {'code': c_code, 'name': c_name, 'rank': rank}
            
            hot_match_str = ", ".join(matched_hot_list) if matched_hot_list else "æ— "
            indicators['ä¸ªè‚¡æ‰€å±æ¿å—'] = f"è¡Œä¸š: {industry_name} | æ¦‚å¿µ: {concept_str} | å‘½ä¸­çƒ­ç‚¹: {hot_match_str}"
            indicators['æ¿å—çƒ­åº¦æŒ‡æ ‡'] = "\n".join(top5_sectors_display) if top5_sectors_display else "æ— æ•°æ®"
            
            # Rank within Sector
            rank_msg = "æ— æ¶¨åœæ•°æ®"
            status_msg = "éæ¿å—é¾™å¤´"
            target_sector_code = None
            target_sector_name = ""
            
            if highest_rank_sector:
                target_sector_code = highest_rank_sector['code']
                target_sector_name = highest_rank_sector['name']
            
            if target_sector_code:
                member_codes = await self._fetch_sector_members(target_sector_code)
                if member_codes and df_limit_all_today is not None:
                    df_sector_limit = df_limit_all_today[df_limit_all_today['ts_code'].isin(member_codes)].copy()
                    if not df_sector_limit.empty:
                        if 'limit_times' in df_sector_limit.columns:
                            df_sector_limit = df_sector_limit.sort_values(['limit_times', 'first_time'], ascending=[False, True])
                            df_sector_limit['rank'] = range(1, len(df_sector_limit) + 1)
                            
                            my_row = df_sector_limit[df_sector_limit['ts_code'] == ts_code]
                            if not my_row.empty:
                                my_rank = int(my_row.iloc[0]['rank'])
                                my_limit = my_row.iloc[0]['limit_times']
                                total = len(df_sector_limit)
                                rank_msg = f"ç¬¬{my_rank}å (æ¿å—:{target_sector_name}, è¿æ¿{my_limit}, æ¶¨åœ{total}å®¶)"
                                
                                if my_rank == 1:
                                    status_msg = "æ¿å—é¾™å¤´ (è¿æ¿é«˜åº¦æœ€é«˜)"
                                elif my_rank <= 3:
                                    status_msg = "æ¿å—å‰æ’ (é¾™äºŒ/é¾™ä¸‰)"
                                else:
                                    status_msg = "æ¿å—è·Ÿé£"
                            else:
                                rank_msg = f"æœªåœ¨{target_sector_name}æ¿å—æ¶¨åœåå•ä¸­"
            else:
                rank_msg = f"æ— çƒ­é—¨å‘½ä¸­ (è¡Œä¸š:{industry_name})"

            indicators['æ¿å—å†…è¿æ¿æ•°æ’å'] = rank_msg
            indicators['ä¸ªè‚¡æ‰€å±æ¿å—è¿æ¿æ¢¯é˜Ÿåœ°ä½'] = status_msg
            
            results.append({
                "code": ts_code,
                "name": name,
                "date": target_date,
                "indicators": indicators
            })

        return {
            "success": True,
            "data": results,
            "metadata": { "query_date": target_date }
        }

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    async def test():
        service = ThsDabanService()
        dates = ['20251209'] # Use a recent date
        names = "åˆ©æ¬§è‚¡ä»½"
        
        for date in dates:
            print(f"\nTesting Daban Analysis for {names} on {date}...")
            result = await service.get_daban_indicators(names, date)
            import json
            print(json.dumps(result, ensure_ascii=False, indent=2))

    asyncio.run(test())
